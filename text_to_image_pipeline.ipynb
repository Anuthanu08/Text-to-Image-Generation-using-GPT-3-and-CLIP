import os
import torch
from models.gpt3 import generate_description
from models.clip_utils import load_clip_model, get_text_embedding, get_image_embedding
from models.vqgan import load_vqgan_model, decode_latent
import matplotlib.pyplot as plt

# Load Models
clip_model, preprocess = load_clip_model()
vqgan = load_vqgan_model()

# Generate Detailed Caption
prompt = "A futuristic cityscape at dusk with neon lights and flying cars"
detailed_caption = generate_description(prompt)
print("Caption:", detailed_caption)

# Get Text Embedding
text_features = get_text_embedding(detailed_caption, clip_model)

# Here, you would run your latent optimization loop.
# assume you have a latent vector:
latent = torch.randn((1, vqgan.latent_dim), device="cuda" if torch.cuda.is_available() else "cpu")
# Decode to image (replace with your optimization loop result)
generated_image = decode_latent(vqgan, latent)

# In [6]: Display the generated image
plt.imshow(generated_image)
plt.axis("off")
plt.show()
